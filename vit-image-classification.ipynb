{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://github.com/lukemelas/PyTorch-Pretrained-ViT/blob/master/pytorch_pretrained_vit/utils.py  \nLoad pretraind vision transformer model and fine tune with medical image.  \nUse positional embedding interpolation (from pre to fine, resolution is 256? -> 384)  \nclass num should be changed to 2  \nbatch size : from 32 to 8  \nlearning rate : 0.0008 -> 0.0002\n<!-- resize_positional_embedding=(image_size != pretrained_image_size) -->\npositional embedding은 weight load시 자동으로 interpolation하도록 구현되어 있음  \nCPU memory increase linearly. need to check memory leakage.  \n(per each epoch, 1~2GB cpu memory more used)  \nits because when image is loaded, it is stacked.  \nfor solving it, make funtion for 1 epoch training.  \nfunction is returned, all stack memory is returned\n\nfor python, function in function, 통용범위는 적용되지만 stack은 바깥쪽 함수가 return될때 회수된다.   \nstack memory saving을 위해서는 function in function이 아닌 각각 function을 별도로 만들자.\n(확인 필요, ram memory 체크할 것, kaggle jupyter notebook의 불안정성이 이유일수도)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd\n!nvidia-smi","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/working\nSun Feb 14 18:26:49 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_pretrained_vit","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting pytorch_pretrained_vit\n  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_vit) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (1.19.5)\nBuilding wheels for collected packages: pytorch-pretrained-vit\n  Building wheel for pytorch-pretrained-vit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-pretrained-vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11131 sha256=719f5d84276b5cb235c0fc294b9a21863b30f03212f0eac256fb2582916c7a43\n  Stored in directory: /root/.cache/pip/wheels/87/1d/d1/c6852ef6d18565e5aee866432ab40c6ffbd3411d592035cddb\nSuccessfully built pytorch-pretrained-vit\nInstalling collected packages: pytorch-pretrained-vit\nSuccessfully installed pytorch-pretrained-vit-0.0.7\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_vit import ViT\nmodel = ViT('B_16_imagenet1k', pretrained=True)","execution_count":5,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\" to /root/.cache/torch/hub/checkpoints/B_16_imagenet1k.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"811b35ab56cf46f4a6586bbf8def7a9a"}},"metadata":{}},{"output_type":"stream","text":"Loaded pretrained weights.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al ../input/dogs-image","execution_count":6,"outputs":[{"output_type":"stream","text":"total 84\r\ndrwxr-xr-x 2 nobody nogroup     0 Feb 14 15:16 .\r\ndrwxr-xr-x 4 root   root     4096 Feb 14 18:26 ..\r\n-rw-r--r-- 1 nobody nogroup 80327 Feb 14 15:16 dog.jpg\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/dogs-image/dog.jpg'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\n\n# Load ViT\nfrom pytorch_pretrained_vit import ViT\nmodel = ViT('B_16_imagenet1k', pretrained=True, num_classes=2)\nmodel.eval()\n\n# Load image\n# NOTE: Assumes an image `img.jpg` exists in the current directory\nimg = transforms.Compose([\n    transforms.Resize((384, 384)), \n    transforms.ToTensor(),\n    transforms.Normalize(0.5, 0.5),\n])(Image.open(path)).unsqueeze(0)\nprint(img.shape) # torch.Size([1, 3, 384, 384])\n\n# Classify\nwith torch.no_grad():\n    outputs = model(img)\nprint(outputs.shape)  # (1, 1000)","execution_count":8,"outputs":[{"output_type":"stream","text":"Loaded pretrained weights.\ntorch.Size([1, 3, 384, 384])\ntorch.Size([1, 2])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(outputs)\nprint(torch.argmax(outputs))","execution_count":9,"outputs":[{"output_type":"stream","text":"tensor(0)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libs\nimport os\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nimport random\nimport json\nimport time\nimport copy\nimport pydicom\nimport torchvision\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torchvision import datasets, transforms\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nfrom pathlib import Path\n\nimport torch.nn.functional as F\n\n# from fastai.conv_learner import *\n# from fastai.dataset import *\n# from fastai.dataset import ImageClassifierData","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/rsna-pneumonia-detection-challenge/'\ndf = pd.read_csv(PATH + 'stage_2_train_labels.csv')\ndf.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"                              patientId      x      y  width  height  Target\n0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patientId</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n      <td>264.0</td>\n      <td>152.0</td>\n      <td>213.0</td>\n      <td>379.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(df.drop_duplicates('patientId').shape)\n# len(list((PATH + 'stage_2_train_images').iterdir()))\nprint(len(os.listdir(PATH + 'stage_2_train_images')))\ndf = df.drop_duplicates('patientId').reset_index(drop=True)\nprint(df.shape)\nprint(df.head())","execution_count":12,"outputs":[{"output_type":"stream","text":"26684\n(26684, 6)\n                              patientId      x      y  width  height  Target\n0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df[0:2000]\ntrain_df, test_df = train_test_split(df, test_size=0.1)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\nprint(train_df.shape)\nprint(test_df.shape)\n\nprint(len(df[df['Target']==1])/len(df))\nprint(len(train_df[train_df['Target']==1])/len(train_df))\nprint(len(test_df[test_df['Target']==1])/len(test_df))\n","execution_count":13,"outputs":[{"output_type":"stream","text":"(24015, 6)\n(2669, 6)\n0.225303552690751\n0.2251509473245888\n0.2266766579243162\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transforms.Compose(transform) if transform else None\n        self.dir = PATH + 'stage_2_train_images/'\n\n    def __len__(self):\n        return len(self.df)\n    \n    def read_dicom_image(self, loc):\n        # return numpy array\n        img_arr = pydicom.read_file(loc).pixel_array\n        img_arr = img_arr/img_arr.max()\n        img_arr = (255*img_arr).clip(0, 255).astype(np.uint8)\n        img_arr = Image.fromarray(img_arr).convert('RGB') # model expects 3 channel image\n        return img_arr    \n\n    def __getitem__(self, idx):\n        pid = self.df.iloc[idx, 0]\n#         print(pid)\n        pimage = self.read_dicom_image(self.dir + pid + '.dcm')\n        if self.transform:\n            pimage = self.transform(pimage)\n        label = self.df.iloc[idx, 5]\n        return pid, pimage, label","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8\n# transform = [transforms.Resize(224), transforms.RandomHorizontalFlip() , transforms.ToTensor()]\ntransform = [transforms.Resize(384), transforms.RandomHorizontalFlip() , transforms.ToTensor()]\n\ntrain_dataset = MDataset(train_df, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\ntest_dataset = MDataset(test_df, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n# focal loss for unbalancing label\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        super(FocalLoss, self).__init__()\n#         self.device = torch.device('cuda')\n#         self.alpha = alpha.to(self.device)\n        self.alpha =alpha\n        self.gamma = gamma\n        self.reduce = reduce\n        self.loss = F.cross_entropy\n#         print(alpha)\n#         print(type(alpha))\n\n    def forward(self, inputs, targets):\n        CE_loss = self.loss(inputs, targets, reduce=False)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * CE_loss\n#         CE_loss = self.loss(inputs, targets, reduce=False, weight=self.alpha)\n#         pt = torch.exp(-(self.loss(inputs, targets, reduce=False)))\n# #         pt = torch.exp(-CE_loss)\n#         F_loss = (1-pt)**self.gamma * CE_loss        \n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss    ","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\nclass F1_Loss(nn.Module):\n    '''Calculate F1 score. Can work with gpu tensors\n    \n    The original implmentation is written by Michal Haltuf on Kaggle.\n    \n    Returns\n    -------\n    torch.Tensor\n        `ndim` == 1. epsilon <= val <= 1\n    \n    Reference\n    ---------\n    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n    '''\n    def __init__(self, epsilon=1e-7):\n        super().__init__()\n        self.epsilon = epsilon\n        \n    def forward(self, y_pred, y_true,):\n        assert y_pred.ndim == 2\n        assert y_true.ndim == 1\n        y_true = F.one_hot(y_true, 2).to(torch.float32)\n        y_pred = F.softmax(y_pred, dim=1)\n        \n        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n\n        precision = tp / (tp + fp + self.epsilon)\n        recall = tp / (tp + fn + self.epsilon)\n\n        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n        return 1 - f1.mean()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# model= torchvision.models.resnet50(pretrained=True)\n\n# num_ftrs = model.fc.in_features\n# model.fc = nn.Linear(num_ftrs, 2) # target label is 2\ncriterion = nn.CrossEntropyLoss()\n# criterion = FocalLoss(alpha=0.97, gamma=2, reduce=True)\n# criterion = F1_Loss()\n# model_ft = model.cuda()\n\n# # save for ensemble\n# default_model = copy.deepcopy(model.state_dict())\n\n# Observe that all parameters are being optimized\noptimizer = optim.Adam(model.parameters(), lr=0.00002)\n\ndev = \"cuda\"\n# dev = \"cpu\"\ndevice = torch.device(dev)\nmodel.to(device)\ncriterion = criterion.to(device)\n\n# # Decay LR by a factor of 0.1 every 7 epochs\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nscheduler = lr_scheduler.LambdaLR(\n    optimizer=optimizer, lr_lambda=lambda epoch: 1 / (epoch + 1)\n)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_eval(model, dev='cuda') :\n    device = torch.device(dev)\n#     test_criterion = F1_Loss()\n    test_criterion = nn.CrossEntropyLoss()\n    test_criterion = test_criterion.to(device)\n    model.to(device)\n    model.train(False)\n    data_loader = test_loader\n    running_corrects = 0\n    test_loss = 0\n    \n    # accuracy and loss\n    with torch.no_grad():    \n        for _, pimages, labels in data_loader:\n            pimages = torch.tensor(pimages)\n            labels = torch.tensor(labels)\n            pimages, labels = pimages.to(device), labels.to(device)  \n            outputs = model(pimages)  \n            loss = test_criterion(outputs, labels)\n            _, preds = torch.max(outputs.data, 1)  \n            running_corrects += torch.sum(preds == labels.data)\n            test_loss += (loss*pimages.size()[0])\n        epoch_acc = running_corrects / len(data_loader.dataset)\n#     print('{} loss: {:.4f} Acc: {:.4f}'.format(\n#         'test',test_loss/len(data_loader.dataset), epoch_acc))   \n    \n    return (epoch_acc, test_loss/len(data_loader.dataset))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_epoch_train(model) :\n    running_loss = 0.0\n    running_corrects = 0\n    count = 0\n    for i, data in enumerate(train_loader):\n        _, pimages, labels = data\n        pimages = torch.tensor(pimages)\n        labels = torch.tensor(labels)\n        pimages, labels = pimages.to(device), labels.to(device)\n\n        outputs = model(pimages)\n        _, preds = torch.max(outputs.data, 1)\n        loss = criterion(outputs, labels)\n#         print(loss)\n\n        # statistics\n        running_loss += (loss) * pimages.size()[0]\n        running_corrects += torch.sum(preds == labels.data) \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step() \n\n        if count%500 == 0 :\n            print('{} th loss {}'.format(i, loss))        \n        if count%1000 == 0 :\n            break\n        count += 1  \n    return running_loss, running_corrects\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ndef model_train(model) :\n    num_epochs = 10\n    # model.train(True)\n    best_test_acc = 0\n    best_f1_loss = 100\n\n    train_loss = []\n    test_loss = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n   \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 50)\n        start = time.time()\n        model.train(True)\n\n#         data_loader = train_loader\n    \n        running_loss,running_corrects = one_epoch_train(model)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_loss.append(float(epoch_loss))\n        epoch_acc = running_corrects / len(train_loader.dataset)\n\n        scheduler.step()\n\n        test_acc, f1_loss = model_eval(model, device)\n        test_loss.append(float(f1_loss))\n\n        print('train Loss: {:.4f} test Loss: {:.4f} train Acc: {:.4f} test Acc: {:.4f}'.format(\n            epoch_loss, f1_loss, epoch_acc, test_acc))  \n        print('time per epoch :', time.time() - start)  \n\n        if test_acc > best_test_acc :\n            best_test_acc = test_acc\n    #         if f1_loss < best_f1_loss :\n    #             best_f1_loss = f1_loss\n            # load best model weights\n            best_model_wts = model.state_dict()\n            print('best model is updated')\n\n        print('-' * 50)\n\n    model.load_state_dict(best_model_wts)\n    test_acc, f1_loss = model_eval(model, device)\n    print('fl_score :', 1-f1_loss)\n    return model, (train_loss, test_loss)\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model, losses = model_train(model)\n\nfrom matplotlib import pyplot as plt\ntrain_lo, test_lo = losses\nepochs = range(10)\n# list(train_lo)\nplt.plot(epochs, train_lo)\n\nplt.plot(epochs, test_lo)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 0/9\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \n","name":"stderr"},{"output_type":"stream","text":"0 th loss 0.6931471824645996\n500 th loss 0.33180946111679077\n1000 th loss 0.21045902371406555\n1500 th loss 0.1784772127866745\n2000 th loss 0.21773290634155273\n2500 th loss 0.24001213908195496\n3000 th loss 0.21444953978061676\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  app.launch_new_instance()\n","name":"stderr"},{"output_type":"stream","text":"train Loss: 0.3867 test Loss: 0.3556 train Acc: 0.8250 test Acc: 0.8430\ntime per epoch : 1572.7995538711548\nbest model is updated\n--------------------------------------------------\nEpoch 1/9\n--------------------------------------------------\n0 th loss 0.7595213055610657\n500 th loss 0.15569430589675903\n1000 th loss 0.09604789316654205\n1500 th loss 0.5787533521652222\n2000 th loss 0.12335128337144852\n2500 th loss 0.3599608540534973\n3000 th loss 0.49028220772743225\ntrain Loss: 0.3414 test Loss: 0.3408 train Acc: 0.8451 test Acc: 0.8494\ntime per epoch : 1560.1536736488342\nbest model is updated\n--------------------------------------------------\nEpoch 2/9\n--------------------------------------------------\n0 th loss 0.2091343104839325\n500 th loss 0.14071549475193024\n1000 th loss 0.9830349683761597\n1500 th loss 0.2950434386730194\n2000 th loss 0.2944294810295105\n2500 th loss 0.5780393481254578\n3000 th loss 0.27363625168800354\ntrain Loss: 0.3200 test Loss: 0.3446 train Acc: 0.8573 test Acc: 0.8415\ntime per epoch : 1566.1833474636078\n--------------------------------------------------\nEpoch 3/9\n--------------------------------------------------\n0 th loss 0.2760779857635498\n500 th loss 0.38842353224754333\n1000 th loss 0.10471487045288086\n1500 th loss 0.17022445797920227\n2000 th loss 0.4275340437889099\n2500 th loss 0.6348000168800354\n3000 th loss 0.13666142523288727\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}