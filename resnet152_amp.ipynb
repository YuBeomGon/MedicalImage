{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://github.com/lukemelas/PyTorch-Pretrained-ViT/blob/master/pytorch_pretrained_vit/utils.py  \nLoad pretraind vision transformer model and fine tune with medical image.  \nUse positional embedding interpolation (from pre to fine, resolution is 256? -> 384)  \nclass num should be changed to 2  \nbatch size : from 16 to 8  \nlearning rate : 0.00008 -> 0.00004\n<!-- resize_positional_embedding=(image_size != pretrained_image_size) -->\npositional embedding은 weight load시 자동으로 interpolation하도록 구현되어 있음  \nCPU memory increase linearly. need to check memory leakage.  \nuse loss.item() instead of loss\n(the computation graph is unintentionally stored somewhere)  \nloss는 tensor 객체이다. 객체가 아닌 value를 더해줘야 한다.\n\nresnet과 비교  \nresnet152  \nbatchsize :16, \nlearning rate : 0.00008, \ngpu mem : 10.3G, \ntraining time : 17 mins, \nRecall 0.5342 precision 0.7273, accuracy 0.8468, F1 score 0.616\n\nresnet152 with amp  \nbatchsize :32,  \nlearning rate : 0.00008, \ngpu mem : 10.5G, \ntraining time : 15 mins, \nRecall 0.645 precision 0.6667, accuracy 0.8441, F1 score 0.6556  \n  \nvit base\nbatchsize :8, \nlearning rate : 0.00004,\ntransformation시 normalization 추가,  \ngpu mem : 10.3G, \ntraining time : 26mins, \nRecall 0.6189 precision 0.6323, accuracy 0.8295, F1 score 0.6255\n\n공통\nloss : focal loss(for train/eval)  \nfine tuning시 resolution 384  \n8 epochs\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd\n!nvidia-smi","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/working\nTue Feb 16 03:33:41 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_pretrained_vit","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting pytorch_pretrained_vit\n  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_vit) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (1.19.5)\nBuilding wheels for collected packages: pytorch-pretrained-vit\n  Building wheel for pytorch-pretrained-vit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-pretrained-vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11131 sha256=429afa90581fc41e4111f97b89e72842a1a60aa642d13361fbdc83232c6e8327\n  Stored in directory: /root/.cache/pip/wheels/87/1d/d1/c6852ef6d18565e5aee866432ab40c6ffbd3411d592035cddb\nSuccessfully built pytorch-pretrained-vit\nInstalling collected packages: pytorch-pretrained-vit\nSuccessfully installed pytorch-pretrained-vit-0.0.7\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al ../input/dogs-image","execution_count":6,"outputs":[{"output_type":"stream","text":"total 84\r\ndrwxr-xr-x 2 nobody nogroup     0 Feb 14 16:02 .\r\ndrwxr-xr-x 4 root   root     4096 Feb 16 03:33 ..\r\n-rw-r--r-- 1 nobody nogroup 80327 Feb 14 16:02 dog.jpg\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/dogs-image/dog.jpg'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import json\n# from PIL import Image\n# import torch\n# from torchvision import transforms\n\n# # Load ViT\n# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True, num_classes=2)\n# model.eval()\n\n# # Load image\n# # NOTE: Assumes an image `img.jpg` exists in the current directory\n# img = transforms.Compose([\n#     transforms.Resize((384, 384)), \n#     transforms.ToTensor(),\n#     transforms.Normalize(0.5, 0.5),\n# ])(Image.open(path)).unsqueeze(0)\n# print(img.shape) # torch.Size([1, 3, 384, 384])\n\n# # Classify\n# with torch.no_grad():\n#     outputs = model(img)\n# print(outputs.shape)  # (1, 1000)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # print(outputs)\n# print(torch.argmax(outputs))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libs\nimport os\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nimport random\nimport json\nimport time\nimport copy\nimport pydicom\nimport torchvision\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torchvision import datasets, transforms\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nfrom pathlib import Path\n\nimport torch.nn.functional as F\n\n# from fastai.conv_learner import *\n# from fastai.dataset import *\n# from fastai.dataset import ImageClassifierData","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/rsna-pneumonia-detection-challenge/'\ndf = pd.read_csv(PATH + 'stage_2_train_labels.csv')\ndf.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"                              patientId      x      y  width  height  Target\n0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patientId</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n      <td>264.0</td>\n      <td>152.0</td>\n      <td>213.0</td>\n      <td>379.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(df.drop_duplicates('patientId').shape)\n# len(list((PATH + 'stage_2_train_images').iterdir()))\nprint(len(os.listdir(PATH + 'stage_2_train_images')))\ndf = df.drop_duplicates('patientId').reset_index(drop=True)\nprint(df.shape)\nprint(df.head())","execution_count":12,"outputs":[{"output_type":"stream","text":"26684\n(26684, 6)\n                              patientId      x      y  width  height  Target\n0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df[0:2000]\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=0)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\nprint(train_df.shape)\nprint(test_df.shape)\n\nprint(len(df[df['Target']==1])/len(df))\nprint(len(train_df[train_df['Target']==1])/len(train_df))\nprint(len(test_df[test_df['Target']==1])/len(test_df))\n","execution_count":13,"outputs":[{"output_type":"stream","text":"(24015, 6)\n(2669, 6)\n0.225303552690751\n0.22477618155319593\n0.23004870738104158\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"                              patientId      x      y  width  height  Target\n0  d0183e7f-16b5-4dec-9190-2141bc78f683    NaN    NaN    NaN     NaN       0\n1  fdeff9e3-54ca-487e-a00b-71c9072eed25    NaN    NaN    NaN     NaN       0\n2  6b033062-62d0-4506-8dda-d3f5df1ee117    NaN    NaN    NaN     NaN       0\n3  9ec108de-18b4-4446-a6e8-4c398f5a6614  209.0  429.0  187.0   204.0       1\n4  f01452c9-636e-47e9-b126-33be9892fdc5    NaN    NaN    NaN     NaN       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patientId</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d0183e7f-16b5-4dec-9190-2141bc78f683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fdeff9e3-54ca-487e-a00b-71c9072eed25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6b033062-62d0-4506-8dda-d3f5df1ee117</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9ec108de-18b4-4446-a6e8-4c398f5a6614</td>\n      <td>209.0</td>\n      <td>429.0</td>\n      <td>187.0</td>\n      <td>204.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f01452c9-636e-47e9-b126-33be9892fdc5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transforms.Compose(transform) if transform else None\n        self.dir = PATH + 'stage_2_train_images/'\n\n    def __len__(self):\n        return len(self.df)\n    \n    def read_dicom_image(self, loc):\n        # return numpy array\n        img_arr = pydicom.read_file(loc).pixel_array\n        img_arr = img_arr/img_arr.max()\n        img_arr = (255*img_arr).clip(0, 255).astype(np.uint8)\n        img_arr = Image.fromarray(img_arr).convert('RGB') # model expects 3 channel image\n        return img_arr    \n\n    def __getitem__(self, idx):\n        pid = self.df.iloc[idx, 0]\n#         print(pid)\n        filepath = [] \n        filepath.append(self.dir) \n        filepath.append(pid) \n        filepath.append('.dcm')\n        filepath = ''.join(filepath)\n        pimage = self.read_dicom_image(filepath)\n        if self.transform:\n            pimage = self.transform(pimage)\n        label = self.df.iloc[idx, 5]\n        return pid, pimage, label","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\ntransform = [transforms.Resize(384), transforms.RandomHorizontalFlip() , transforms.ToTensor()]\n# transform = [transforms.Resize(384), transforms.RandomHorizontalFlip() , transforms.ToTensor(), \n#             transforms.Normalize(0.5, 0.5)]\n\ntrain_dataset = MDataset(train_df, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\ntest_dataset = MDataset(test_df, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n# focal loss for unbalancing label\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        super(FocalLoss, self).__init__()\n#         self.device = torch.device('cuda')\n#         self.alpha = alpha.to(self.device)\n        self.alpha =alpha\n        self.gamma = gamma\n        self.reduce = reduce\n        self.loss = F.cross_entropy\n#         print(alpha)\n#         print(type(alpha))\n\n    def forward(self, inputs, targets):\n        CE_loss = self.loss(inputs, targets, reduce=False)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * CE_loss\n#         CE_loss = self.loss(inputs, targets, reduce=False, weight=self.alpha)\n#         pt = torch.exp(-(self.loss(inputs, targets, reduce=False)))\n# #         pt = torch.exp(-CE_loss)\n#         F_loss = (1-pt)**self.gamma * CE_loss        \n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss    ","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\nclass F1_Loss(nn.Module):\n    '''Calculate F1 score. Can work with gpu tensors\n    \n    The original implmentation is written by Michal Haltuf on Kaggle.\n    \n    Returns\n    -------\n    torch.Tensor\n        `ndim` == 1. epsilon <= val <= 1\n    \n    Reference\n    ---------\n    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n    '''\n    def __init__(self, epsilon=1e-7):\n        super().__init__()\n        self.epsilon = epsilon\n        \n    def forward(self, y_pred, y_true,):\n        assert y_pred.ndim == 2\n        assert y_true.ndim == 1\n        y_true = F.one_hot(y_true, 2).to(torch.float32)\n        y_pred = F.softmax(y_pred, dim=1)\n        \n        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n\n        precision = tp / (tp + fp + self.epsilon)\n        recall = tp / (tp + fn + self.epsilon)\n\n        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n        return 1 - f1.mean()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel= torchvision.models.resnet152(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2) # target label is 2\n\n# # Load ViT\n# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True, num_classes=2)\n\n# criterion = nn.CrossEntropyLoss()\ncriterion = FocalLoss(alpha=0.97, gamma=2, reduce=True)\n# criterion = F1_Loss()\n# model_ft = model.cuda()\n\n# # save for ensemble\n# default_model = copy.deepcopy(model.state_dict())\n\n# Observe that all parameters are being optimized\noptimizer = optim.Adam(model.parameters(), lr=0.00008)\n\ndev = \"cuda\"\n# dev = \"cpu\"\ndevice = torch.device(dev)\nmodel.to(device)\n\ncriterion = criterion.to(device)\n\n# # Decay LR by a factor of 0.1 every 7 epochs\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nscheduler = lr_scheduler.LambdaLR(\n    optimizer=optimizer, lr_lambda=lambda epoch: 1 / (epoch + 1)\n)\n\nscaler = torch.cuda.amp.GradScaler()","execution_count":19,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd1fd94164446a489eaea981b84c487"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_eval(model, criterion, dev='cuda' ) :\n#     print('evaluation mode')\n    device = torch.device(dev)\n    criterion = criterion.to(device)\n    model.to(device)\n    model.train(False)\n    data_loader = test_loader\n    running_corrects = 0\n    test_loss = 0\n    \n    # accuracy and loss\n    with torch.no_grad():    \n        for _, pimages, labels in data_loader:\n#             pimages = torch.tensor(pimages)\n#             labels = torch.tensor(labels)\n            pimages, labels = pimages.to(device), labels.to(device)  \n            outputs = model(pimages)  \n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs.data, 1)  \n            running_corrects += torch.sum(preds == labels.data)\n            test_loss += loss.item()*pimages.size()[0]\n        epoch_acc = running_corrects / len(data_loader.dataset)\n#     print('{} loss: {:.4f} Acc: {:.4f}'.format(\n#         'test',test_loss/len(data_loader.dataset), epoch_acc))   \n    \n    return (epoch_acc, test_loss/len(data_loader.dataset))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_epoch_train(model, optimizer, criterion) :\n    running_loss = 0.0\n    running_corrects = 0\n    for i, data in enumerate(train_loader):\n        optimizer.zero_grad()\n        _, pimages, labels = data\n#         pimages = torch.tensor(pimages)\n#         labels = torch.tensor(labels)\n#         pimages = labels.clone().detach()\n        labels = labels.clone().detach()\n        pimages, labels = pimages.to(device), labels.to(device)\n\n        with torch.cuda.amp.autocast():\n            outputs = model(pimages)\n            loss = criterion(outputs, labels)\n        _, preds = torch.max(outputs.data, 1)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # statistics\n        running_loss += loss.item() * pimages.size()[0]\n        running_corrects += torch.sum(preds == labels.data) \n\n#         loss.backward()\n#         optimizer.step()   \n      \n    return running_loss, running_corrects\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ndef model_train(model, optimizer, criterion, scheduler) :  \n    # model.train(True)\n    best_test_acc = 0\n    best_f1_loss = 100\n\n    train_loss = []\n    test_loss = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n   \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 50)\n        start = time.time()\n        model.train(True)\n    \n        running_loss,running_corrects = one_epoch_train(model, optimizer, criterion)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_loss.append(float(epoch_loss))\n        epoch_acc = running_corrects / len(train_loader.dataset)\n\n        scheduler.step()\n\n        test_acc, f1_loss = model_eval(model, criterion, device)\n        test_loss.append(float(f1_loss))\n\n        print('train Loss: {:.4f} test Loss: {:.4f} train Acc: {:.4f} test Acc: {:.4f}'.format(\n            epoch_loss, f1_loss, epoch_acc, test_acc))  \n        print('time per epoch :', time.time() - start)  \n\n#         if test_acc > best_test_acc :\n#             best_test_acc = test_acc\n        if f1_loss < best_f1_loss :\n            best_f1_loss = f1_loss\n            # load best model weights\n            best_model_wts = model.state_dict()\n            print('best model is updated')\n\n        print('-' * 50)\n\n    model.load_state_dict(best_model_wts)\n#     test_acc, f1_loss = model_eval(model, criterion, device)\n#     print('fl_score :', 1-f1_loss)\n    return model, (train_loss, test_loss)\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 5\n%time model, losses = model_train(model, optimizer, criterion, scheduler)\n\nfrom matplotlib import pyplot as plt\ntrain_lo, test_lo = losses\nepochs = range(num_epochs)\n# list(train_lo)\nplt.plot(epochs, train_lo)\n\nplt.plot(epochs, test_lo)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":23,"outputs":[{"output_type":"stream","text":"Epoch 0/4\n--------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n  warnings.warn(warning.format(ret))\n","name":"stderr"},{"output_type":"stream","text":"train Loss: 0.1004 test Loss: 0.1001 train Acc: 0.8196 test Acc: 0.8123\ntime per epoch : 903.4814238548279\nbest model is updated\n--------------------------------------------------\nEpoch 1/4\n--------------------------------------------------\ntrain Loss: 0.0868 test Loss: 0.0978 train Acc: 0.8443 test Acc: 0.8344\ntime per epoch : 874.4703905582428\nbest model is updated\n--------------------------------------------------\nEpoch 2/4\n--------------------------------------------------\ntrain Loss: 0.0776 test Loss: 0.0847 train Acc: 0.8624 test Acc: 0.8501\ntime per epoch : 874.3014712333679\nbest model is updated\n--------------------------------------------------\nEpoch 3/4\n--------------------------------------------------\ntrain Loss: 0.0658 test Loss: 0.0899 train Acc: 0.8851 test Acc: 0.8456\ntime per epoch : 880.0817949771881\n--------------------------------------------------\nEpoch 4/4\n--------------------------------------------------\ntrain Loss: 0.0482 test Loss: 0.1073 train Acc: 0.9258 test Acc: 0.8434\ntime per epoch : 874.8345730304718\n--------------------------------------------------\nCPU times: user 38min 9s, sys: 9min 7s, total: 47min 16s\nWall time: 1h 13min 27s\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"label 비가 1:4로 unbalance하다.  \nmodel saving의 기준을 test accuracy가 아닌 f1 score로 바꾸었다.  \ntrain 및 test loss 모두 f1 loss이용  \n\n그러나 F1 loss를 쓸 경우 학습 속도가 느리다.  \nunder fit 발생  \nfocal loss를 쓰는 것이 더 좋을 듯.."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(False)\ndata_loader = test_loader\nrunning_corrects = 0\npreds = []\n\n# accuracy and loss\nwith torch.no_grad():   \n    for _, pimages, labels in data_loader:\n        pimages, labels = pimages.to(device), labels.to(device) \n        outputs = model(pimages)\n        _, p = torch.max(outputs.data, 1)\n        preds.append(p)\n        \nanswer = []\nfor a,in zip(preds) :\n    for i in a :\n        answer.append((int(i)))\n# print(answer)        ","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix and F1 score of ensenble\nTP, FP, TN, FN = 0,0,0,0\nfor a, p in zip(test_df.Target, answer) :\n    if a == 1 and p == 1 :\n        TP += 1\n    elif a == 1 and p == 0 :\n        FN += 1\n    elif a == 0 and p == 1 :\n        FP += 1\n    elif a == 0 and p == 0 :\n        TN += 1\n\nprint('TP {}, FP {}, TN {}, FN {}'.format(TP, FP, TN, FN))   \nRecall = TP / (TP + FN)\nPrecision = TP / (TP + FP)\nAccuracy = (TP + TN) / (TP + FP + FN + TN)\nF1_Score = 2 * (Recall * Precision) / (Recall + Precision)\nprint('Recall {:.4} precision {:.4}, accuracy {:.4}, F1 score {:.4}'.format(\n    Recall, Precision, Accuracy, F1_Score))\n","execution_count":25,"outputs":[{"output_type":"stream","text":"TP 396, FP 198, TN 1857, FN 218\nRecall 0.645 precision 0.6667, accuracy 0.8441, F1 score 0.6556\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}