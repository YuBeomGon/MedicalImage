{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://github.com/lukemelas/PyTorch-Pretrained-ViT/blob/master/pytorch_pretrained_vit/utils.py  \nLoad pretraind vision transformer model and fine tune with medical image.  \nUse positional embedding interpolation (from pre to fine, resolution is 256? -> 384)  \nclass num should be changed to 2  \nbatch size : from 32 to 8  \nlearning rate : 0.00008 -> 0.00002\n<!-- resize_positional_embedding=(image_size != pretrained_image_size) -->\npositional embedding은 weight load시 자동으로 interpolation하도록 구현되어 있음  \nCPU memory increase linearly. need to check memory leakage.  \nuse loss.item() instead of loss\n(the computation graph is unintentionally stored somewhere)  \nloss는 tensor 객체이다. 객체가 아닌 value를 더해줘야 한다.\n\nresnet과 비교  \nresnet152  \nbatchsize :16, \nlearning rate : 0.00008, \ngpu mem : 10.3G, \ntraining time : 17 mins, \nF1 score, accuracy : , \n  \nvit base\n\n\n공통\nloss : focal loss(for train/eval)  \nfine tuning시 resolution 384  \n8 epochs\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd\n!nvidia-smi","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/working\nMon Feb 15 13:27:39 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_pretrained_vit","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting pytorch_pretrained_vit\n  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch_pretrained_vit) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pytorch_pretrained_vit) (1.19.5)\nBuilding wheels for collected packages: pytorch-pretrained-vit\n  Building wheel for pytorch-pretrained-vit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-pretrained-vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11131 sha256=a94eb64d75158679c10caea1f107054b059df741bffd51ec879c892bb4f42878\n  Stored in directory: /root/.cache/pip/wheels/87/1d/d1/c6852ef6d18565e5aee866432ab40c6ffbd3411d592035cddb\nSuccessfully built pytorch-pretrained-vit\nInstalling collected packages: pytorch-pretrained-vit\nSuccessfully installed pytorch-pretrained-vit-0.0.7\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al ../input/dogs-image","execution_count":6,"outputs":[{"output_type":"stream","text":"total 84\r\ndrwxr-xr-x 2 nobody nogroup     0 Feb 14 16:02 .\r\ndrwxr-xr-x 4 root   root     4096 Feb 15 13:27 ..\r\n-rw-r--r-- 1 nobody nogroup 80327 Feb 14 16:02 dog.jpg\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/dogs-image/dog.jpg'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import json\n# from PIL import Image\n# import torch\n# from torchvision import transforms\n\n# # Load ViT\n# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True, num_classes=2)\n# model.eval()\n\n# # Load image\n# # NOTE: Assumes an image `img.jpg` exists in the current directory\n# img = transforms.Compose([\n#     transforms.Resize((384, 384)), \n#     transforms.ToTensor(),\n#     transforms.Normalize(0.5, 0.5),\n# ])(Image.open(path)).unsqueeze(0)\n# print(img.shape) # torch.Size([1, 3, 384, 384])\n\n# # Classify\n# with torch.no_grad():\n#     outputs = model(img)\n# print(outputs.shape)  # (1, 1000)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # print(outputs)\n# print(torch.argmax(outputs))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libs\nimport os\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nimport random\nimport json\nimport time\nimport copy\nimport pydicom\nimport torchvision\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torchvision import datasets, transforms\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nfrom pathlib import Path\n\nimport torch.nn.functional as F\n\n# from fastai.conv_learner import *\n# from fastai.dataset import *\n# from fastai.dataset import ImageClassifierData","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/rsna-pneumonia-detection-challenge/'\ndf = pd.read_csv(PATH + 'stage_2_train_labels.csv')\ndf.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"                              patientId      x      y  width  height  Target\n0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patientId</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n      <td>264.0</td>\n      <td>152.0</td>\n      <td>213.0</td>\n      <td>379.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(df.drop_duplicates('patientId').shape)\n# len(list((PATH + 'stage_2_train_images').iterdir()))\nprint(len(os.listdir(PATH + 'stage_2_train_images')))\ndf = df.drop_duplicates('patientId').reset_index(drop=True)\nprint(df.shape)\nprint(df.head())","execution_count":12,"outputs":[{"output_type":"stream","text":"26684\n(26684, 6)\n                              patientId      x      y  width  height  Target\n0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df[0:2000]\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=0)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\nprint(train_df.shape)\nprint(test_df.shape)\n\nprint(len(df[df['Target']==1])/len(df))\nprint(len(train_df[train_df['Target']==1])/len(train_df))\nprint(len(test_df[test_df['Target']==1])/len(test_df))\n","execution_count":13,"outputs":[{"output_type":"stream","text":"(24015, 6)\n(2669, 6)\n0.225303552690751\n0.22477618155319593\n0.23004870738104158\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"                              patientId      x      y  width  height  Target\n0  d0183e7f-16b5-4dec-9190-2141bc78f683    NaN    NaN    NaN     NaN       0\n1  fdeff9e3-54ca-487e-a00b-71c9072eed25    NaN    NaN    NaN     NaN       0\n2  6b033062-62d0-4506-8dda-d3f5df1ee117    NaN    NaN    NaN     NaN       0\n3  9ec108de-18b4-4446-a6e8-4c398f5a6614  209.0  429.0  187.0   204.0       1\n4  f01452c9-636e-47e9-b126-33be9892fdc5    NaN    NaN    NaN     NaN       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patientId</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d0183e7f-16b5-4dec-9190-2141bc78f683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fdeff9e3-54ca-487e-a00b-71c9072eed25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6b033062-62d0-4506-8dda-d3f5df1ee117</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9ec108de-18b4-4446-a6e8-4c398f5a6614</td>\n      <td>209.0</td>\n      <td>429.0</td>\n      <td>187.0</td>\n      <td>204.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>f01452c9-636e-47e9-b126-33be9892fdc5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transforms.Compose(transform) if transform else None\n        self.dir = PATH + 'stage_2_train_images/'\n\n    def __len__(self):\n        return len(self.df)\n    \n    def read_dicom_image(self, loc):\n        # return numpy array\n        img_arr = pydicom.read_file(loc).pixel_array\n        img_arr = img_arr/img_arr.max()\n        img_arr = (255*img_arr).clip(0, 255).astype(np.uint8)\n        img_arr = Image.fromarray(img_arr).convert('RGB') # model expects 3 channel image\n        return img_arr    \n\n    def __getitem__(self, idx):\n        pid = self.df.iloc[idx, 0]\n#         print(pid)\n        filepath = [] \n        filepath.append(self.dir) \n        filepath.append(pid) \n        filepath.append('.dcm')\n        filepath = ''.join(filepath)\n        pimage = self.read_dicom_image(filepath)\n        if self.transform:\n            pimage = self.transform(pimage)\n        label = self.df.iloc[idx, 5]\n        return pid, pimage, label","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\ntransform = [transforms.Resize(384), transforms.RandomHorizontalFlip() , transforms.ToTensor()]\n# transform = [transforms.Resize(384), transforms.RandomHorizontalFlip() , transforms.ToTensor(), \n#             transforms.Normalize(0.5, 0.5)]\n\ntrain_dataset = MDataset(train_df, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\ntest_dataset = MDataset(test_df, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n# focal loss for unbalancing label\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        super(FocalLoss, self).__init__()\n#         self.device = torch.device('cuda')\n#         self.alpha = alpha.to(self.device)\n        self.alpha =alpha\n        self.gamma = gamma\n        self.reduce = reduce\n        self.loss = F.cross_entropy\n#         print(alpha)\n#         print(type(alpha))\n\n    def forward(self, inputs, targets):\n        CE_loss = self.loss(inputs, targets, reduce=False)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * CE_loss\n#         CE_loss = self.loss(inputs, targets, reduce=False, weight=self.alpha)\n#         pt = torch.exp(-(self.loss(inputs, targets, reduce=False)))\n# #         pt = torch.exp(-CE_loss)\n#         F_loss = (1-pt)**self.gamma * CE_loss        \n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss    ","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\nclass F1_Loss(nn.Module):\n    '''Calculate F1 score. Can work with gpu tensors\n    \n    The original implmentation is written by Michal Haltuf on Kaggle.\n    \n    Returns\n    -------\n    torch.Tensor\n        `ndim` == 1. epsilon <= val <= 1\n    \n    Reference\n    ---------\n    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n    '''\n    def __init__(self, epsilon=1e-7):\n        super().__init__()\n        self.epsilon = epsilon\n        \n    def forward(self, y_pred, y_true,):\n        assert y_pred.ndim == 2\n        assert y_true.ndim == 1\n        y_true = F.one_hot(y_true, 2).to(torch.float32)\n        y_pred = F.softmax(y_pred, dim=1)\n        \n        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n\n        precision = tp / (tp + fp + self.epsilon)\n        recall = tp / (tp + fn + self.epsilon)\n\n        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n        return 1 - f1.mean()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel= torchvision.models.resnet152(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2) # target label is 2\n\n# # Load ViT\n# from pytorch_pretrained_vit import ViT\n# model = ViT('B_16_imagenet1k', pretrained=True, num_classes=2)\n\n# criterion = nn.CrossEntropyLoss()\n# criterion = FocalLoss(alpha=0.97, gamma=2, reduce=True)\ncriterion = F1_Loss()\n# model_ft = model.cuda()\n\n# # save for ensemble\n# default_model = copy.deepcopy(model.state_dict())\n\n# Observe that all parameters are being optimized\noptimizer = optim.Adam(model.parameters(), lr=0.00008)\n\ndev = \"cuda\"\n# dev = \"cpu\"\ndevice = torch.device(dev)\nmodel.to(device)\n\ncriterion = criterion.to(device)\n\n# # Decay LR by a factor of 0.1 every 7 epochs\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nscheduler = lr_scheduler.LambdaLR(\n    optimizer=optimizer, lr_lambda=lambda epoch: 1 / (epoch + 1)\n)","execution_count":19,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36885c6bdd4d46969b49b520346d3f03"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_eval(model, dev='cuda', criterion ) :\n#     print('evaluation mode')\n    device = torch.device(dev)\n    criterion = criterion.to(device)\n    model.to(device)\n    model.train(False)\n    data_loader = test_loader\n    running_corrects = 0\n    test_loss = 0\n    \n    # accuracy and loss\n    with torch.no_grad():    \n        for _, pimages, labels in data_loader:\n#             pimages = torch.tensor(pimages)\n#             labels = torch.tensor(labels)\n            pimages, labels = pimages.to(device), labels.to(device)  \n            outputs = model(pimages)  \n            loss = test_criterion(outputs, labels)\n            _, preds = torch.max(outputs.data, 1)  \n            running_corrects += torch.sum(preds == labels.data)\n            test_loss += loss.item()*pimages.size()[0]\n        epoch_acc = running_corrects / len(data_loader.dataset)\n#     print('{} loss: {:.4f} Acc: {:.4f}'.format(\n#         'test',test_loss/len(data_loader.dataset), epoch_acc))   \n    \n    return (epoch_acc, test_loss/len(data_loader.dataset))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_epoch_train(model, optimizer, criterion) :\n    running_loss = 0.0\n    running_corrects = 0\n    for i, data in enumerate(train_loader):\n        _, pimages, labels = data\n#         pimages = torch.tensor(pimages)\n#         labels = torch.tensor(labels)\n#         pimages = labels.clone().detach()\n        labels = labels.clone().detach()\n        pimages, labels = pimages.to(device), labels.to(device)\n\n        outputs = model(pimages)\n        _, preds = torch.max(outputs.data, 1)\n        loss = criterion(outputs, labels)\n\n        # statistics\n        running_loss += loss.item() * pimages.size()[0]\n        running_corrects += torch.sum(preds == labels.data) \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()   \n      \n    return running_loss, running_corrects\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ndef model_train(model, optimizer, criterion, scheduler) :  \n    # model.train(True)\n    best_test_acc = 0\n    best_f1_loss = 100\n\n    train_loss = []\n    test_loss = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n   \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 50)\n        start = time.time()\n        model.train(True)\n    \n        running_loss,running_corrects = one_epoch_train(model, optimizer, criterion)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_loss.append(float(epoch_loss))\n        epoch_acc = running_corrects / len(train_loader.dataset)\n\n        scheduler.step()\n\n        test_acc, f1_loss = model_eval(model, device, criterion)\n        test_loss.append(float(f1_loss))\n\n        print('train Loss: {:.4f} test Loss: {:.4f} train Acc: {:.4f} test Acc: {:.4f}'.format(\n            epoch_loss, f1_loss, epoch_acc, test_acc))  \n        print('time per epoch :', time.time() - start)  \n\n#         if test_acc > best_test_acc :\n#             best_test_acc = test_acc\n        if f1_loss < best_f1_loss :\n            best_f1_loss = f1_loss\n            # load best model weights\n            best_model_wts = model.state_dict()\n            print('best model is updated')\n\n        print('-' * 50)\n\n    model.load_state_dict(best_model_wts)\n    test_acc, f1_loss = model_eval(model, device)\n    print('fl_score :', 1-f1_loss)\n    return model, (train_loss, test_loss)\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 8\nmodel, losses = model_train(model, optimizer, criterion, scheduler)\n\nfrom matplotlib import pyplot as plt\ntrain_lo, test_lo = losses\nepochs = range(num_epochs)\n# list(train_lo)\nplt.plot(epochs, train_lo)\n\nplt.plot(epochs, test_lo)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":23,"outputs":[{"output_type":"stream","text":"Epoch 0/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.3027 test Loss: 0.2770 train Acc: 0.7958 test Acc: 0.8130\ntime per epoch : 1014.7199609279633\nbest model is updated\n--------------------------------------------------\nEpoch 1/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2755 test Loss: 0.2737 train Acc: 0.8147 test Acc: 0.8265\ntime per epoch : 1009.0020573139191\nbest model is updated\n--------------------------------------------------\nEpoch 2/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2619 test Loss: 0.2741 train Acc: 0.8215 test Acc: 0.8273\ntime per epoch : 1010.6304368972778\n--------------------------------------------------\nEpoch 3/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2543 test Loss: 0.2643 train Acc: 0.8288 test Acc: 0.8295\ntime per epoch : 1011.8285353183746\nbest model is updated\n--------------------------------------------------\nEpoch 4/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2541 test Loss: 0.2575 train Acc: 0.8305 test Acc: 0.8393\ntime per epoch : 1009.2008771896362\nbest model is updated\n--------------------------------------------------\nEpoch 5/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2492 test Loss: 0.2570 train Acc: 0.8332 test Acc: 0.8423\ntime per epoch : 1010.4934732913971\nbest model is updated\n--------------------------------------------------\nEpoch 6/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2480 test Loss: 0.2532 train Acc: 0.8317 test Acc: 0.8306\ntime per epoch : 1008.8929626941681\nbest model is updated\n--------------------------------------------------\nEpoch 7/7\n--------------------------------------------------\nevaluation mode\ntrain Loss: 0.2469 test Loss: 0.2489 train Acc: 0.8372 test Acc: 0.8344\ntime per epoch : 1010.3849375247955\nbest model is updated\n--------------------------------------------------\nevaluation mode\nfl_score : 0.749226579508919\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (10,) and (8,)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-56b2d498063a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# list(train_lo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_lo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m     return gca().plot(\n\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (8,)"]},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\ntrain_lo, test_lo = losses\nepochs = range(8)\n# list(train_lo)\nplt.plot(epochs, train_lo)\n\nplt.plot(epochs, test_lo)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"label 비가 1:4로 unbalance하다.  \nmodel saving의 기준을 test accuracy가 아닌 f1 score로 바꾸었다.  \ntrain 및 test loss 모두 f1 loss이용  \n\n그러나 F1 loss를 쓸 경우 학습 속도가 느리다.  \nunder fit 발생  \nfocal loss를 쓰는 것이 더 좋을 듯.."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(False)\ndata_loader = test_loader\nrunning_corrects = 0\npreds = []\n\n# accuracy and loss\nwith torch.no_grad():   \n    for _, pimages, labels in data_loader:\n        pimages, labels = pimages.to(device), labels.to(device) \n        outputs = model(pimages)\n        _, p = torch.max(outputs.data, 1)\n        preds.append(p)\n        \nanswer = []\nfor a,in zip(preds) :\n    for i in a :\n        answer.append((int(i)))\n# print(answer)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix and F1 score of ensenble\nTP, FP, TN, FN = 0,0,0,0\nfor a, p in zip(test_df.Target, answer) :\n    if a == 1 and p == 1 :\n        TP += 1\n    elif a == 1 and p == 0 :\n        FN += 1\n    elif a == 0 and p == 1 :\n        FP += 1\n    elif a == 0 and p == 0 :\n        TN += 1\n\nprint('TP {}, FP {}, TN {}, FN {}'.format(TP, FP, TN, FN))   \nRecall = TP / (TP + FN)\nPrecision = TP / (TP + FP)\nAccuracy = (TP + TN) / (TP + FP + FN + TN)\nF1_Score = 2 * (Recall * Precision) / (Recall + Precision)\nprint('Recall {:.4} precision {:.4}, accuracy {:.4}, F1 score {:.4}'.format(\n    Recall, Precision, Accuracy, F1_Score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}